{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e99562-92d5-43ae-bd2c-e3afe3be8c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install the required tf-keras package\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tf_keras.layers import Dense, LSTM, Input, Dropout, Bidirectional\n",
    "from tf_keras.models import Model\n",
    "import logging\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='[%(asctime)s] [%(levelname)s]: %(message)s')\n",
    "log = logging.info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa09e55c-db53-443b-a38f-c85081019403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:17:04,991] [INFO]: Loading final features dataset...\n",
      "C:\\Users\\manav\\AppData\\Local\\Temp\\ipykernel_11588\\3640701438.py:10: DtypeWarning: Columns (47,48,49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  final_features = pd.read_csv(final_features_path)\n",
      "[2024-08-02 18:17:07,591] [INFO]: Original final features dataset loaded with shape: (1443182, 50)\n",
      "[2024-08-02 18:17:07,592] [INFO]: Reduced final features dataset shape: (5000, 50)\n",
      "[2024-08-02 18:17:07,592] [INFO]: First 15 rows of the reduced final features dataset:\n",
      "[2024-08-02 18:17:07,604] [INFO]: Loading final script features dataset...\n",
      "[2024-08-02 18:17:07,628] [INFO]: Final script features dataset loaded with shape: (930, 3)\n",
      "[2024-08-02 18:17:07,628] [INFO]: First 15 rows of the final script features dataset:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    averageRating  numVotes  runtimeMinutes_normalized  startYear  \\\n",
      "0             5.7      2063                   0.000017     1894.0   \n",
      "1             5.6       279                   0.000084     1892.0   \n",
      "2             6.5      2038                   0.000084     1892.0   \n",
      "3             5.4       180                   0.000202     1892.0   \n",
      "4             6.2      2798                   0.000017     1893.0   \n",
      "5             5.0       191                   0.000017     1894.0   \n",
      "6             5.4       878                   0.000017     1894.0   \n",
      "7             5.4      2210                   0.000017     1894.0   \n",
      "8             5.4       212                   0.000757     1894.0   \n",
      "9             6.8      7633                   0.000017     1895.0   \n",
      "10            5.2       391                   0.000017     1895.0   \n",
      "11            7.4     12998                   0.000017     1896.0   \n",
      "12            5.7      1983                   0.000017     1895.0   \n",
      "13            7.1      5902                   0.000017     1895.0   \n",
      "14            6.1      1193                   0.000034     1894.0   \n",
      "\n",
      "    titleType_movie  titleType_short  titleType_tvEpisode  \\\n",
      "0             False             True                False   \n",
      "1             False             True                False   \n",
      "2             False             True                False   \n",
      "3             False             True                False   \n",
      "4             False             True                False   \n",
      "5             False             True                False   \n",
      "6             False             True                False   \n",
      "7             False             True                False   \n",
      "8              True            False                False   \n",
      "9             False             True                False   \n",
      "10            False             True                False   \n",
      "11            False             True                False   \n",
      "12            False             True                False   \n",
      "13            False             True                False   \n",
      "14            False             True                False   \n",
      "\n",
      "    titleType_tvMiniSeries  titleType_tvMovie  titleType_tvSeries  ...  war  \\\n",
      "0                    False              False               False  ...    0   \n",
      "1                    False              False               False  ...    0   \n",
      "2                    False              False               False  ...    0   \n",
      "3                    False              False               False  ...    0   \n",
      "4                    False              False               False  ...    0   \n",
      "5                    False              False               False  ...    0   \n",
      "6                    False              False               False  ...    0   \n",
      "7                    False              False               False  ...    0   \n",
      "8                    False              False               False  ...    0   \n",
      "9                    False              False               False  ...    0   \n",
      "10                   False              False               False  ...    0   \n",
      "11                   False              False               False  ...    0   \n",
      "12                   False              False               False  ...    0   \n",
      "13                   False              False               False  ...    0   \n",
      "14                   False              False               False  ...    0   \n",
      "\n",
      "    western  season_Winter  directorPopularity  wordCount  sentiment  \\\n",
      "0         0           True                 0.0        0.0   0.000000   \n",
      "1         0           True                 0.0        0.0   0.000000   \n",
      "2         0           True                 0.0        0.0   0.000000   \n",
      "3         0           True                 0.0        0.0   0.000000   \n",
      "4         0           True                 0.0        0.0   0.000000   \n",
      "5         0           True                 0.0    16308.0   0.007363   \n",
      "6         0           True                 0.0        0.0   0.000000   \n",
      "7         0           True                 0.0        0.0   0.000000   \n",
      "8         0           True                 0.0    20288.0  -0.003107   \n",
      "9         0           True                 0.0        0.0   0.000000   \n",
      "10        0           True                 0.0        0.0   0.000000   \n",
      "11        0           True                 0.0        0.0   0.000000   \n",
      "12        0           True                 0.0    21047.0  -0.029082   \n",
      "13        0           True                 0.0        0.0   0.000000   \n",
      "14        0           True                 0.0        0.0   0.000000   \n",
      "\n",
      "    readabilityScore  comedy_indicator  horror_indicator  action_indicator  \n",
      "0           0.000000             False             False             False  \n",
      "1           0.000000             False             False             False  \n",
      "2           0.000000             False             False             False  \n",
      "3           0.000000             False             False             False  \n",
      "4           0.000000             False             False             False  \n",
      "5          70.154868              True              True              True  \n",
      "6           0.000000             False             False             False  \n",
      "7           0.000000             False             False             False  \n",
      "8          87.729438              True              True              True  \n",
      "9           0.000000             False             False             False  \n",
      "10          0.000000             False             False             False  \n",
      "11          0.000000             False             False             False  \n",
      "12         85.510201              True              True              True  \n",
      "13          0.000000             False             False             False  \n",
      "14          0.000000             False             False             False  \n",
      "\n",
      "[15 rows x 50 columns]\n",
      "                              tokenized_script_padded  sentiment  \\\n",
      "0   [  101 15969 10225 25318  1005  5034  1041  10...  -0.008808   \n",
      "1   [  101  2260  2086  1037  6658 15773  2011  21...   0.050902   \n",
      "2   [  101 14387 16843 15773  2011  4202 13243  10...   0.008484   \n",
      "3   [  101  2258  7966  1005  1055  2154 15773  20...   0.019702   \n",
      "4   [  101 26794 11136  2011  8825  2879 21456 138...   0.034180   \n",
      "5   [  101 12098 16313 24449 15773  2011  6141 157...   0.042304   \n",
      "6   [  101 10877  2011  2585  1055  1012  2175 105...   0.006156   \n",
      "7   [  101 10162  2630  2011  5811 18832  9516  70...  -0.020476   \n",
      "8   [  101 12098  3995 15773  2011  3782 26568  20...   0.025774   \n",
      "9   [  101  2849 18655  5280  7011  3207  1999 285...   0.032139   \n",
      "10  [  101  1996  2390  1997  4768  3200  1997  10...  -0.113297   \n",
      "11  [  101 29596  1998  2214 12922 15773  2011 103...   0.066220   \n",
      "12  [  101 13029  2847 15773  2011  4079 17935 148...   0.046779   \n",
      "13  [  101  4300 15773  2011  2848  3016 20465 107...   0.057808   \n",
      "14  [  101  1996  3063 15773  2011  8709  5292 134...   0.065031   \n",
      "\n",
      "    readabilityScore  \n",
      "0              87.92  \n",
      "1              78.45  \n",
      "2              80.68  \n",
      "3              79.56  \n",
      "4              88.13  \n",
      "5              71.71  \n",
      "6              79.97  \n",
      "7              78.45  \n",
      "8              70.70  \n",
      "9              79.87  \n",
      "10             87.72  \n",
      "11             80.17  \n",
      "12             85.18  \n",
      "13             80.17  \n",
      "14             83.76  \n"
     ]
    }
   ],
   "source": [
    "# Define data paths\n",
    "base_dir = 'D:\\\\manav\\\\Documents\\\\Engineering\\\\Masters\\\\EE8206\\\\Project\\\\MovieSuccessPredictor'\n",
    "processed_data_dir = os.path.join(base_dir, 'data', 'processed')\n",
    "final_features_path = os.path.join(processed_data_dir, 'final_features.csv')\n",
    "final_script_features_path = os.path.join(processed_data_dir, 'final_script_features.csv')\n",
    "\n",
    "# Load the final features dataset\n",
    "log(\"Loading final features dataset...\")\n",
    "try:\n",
    "    final_features = pd.read_csv(final_features_path)\n",
    "    log(f\"Original final features dataset loaded with shape: {final_features.shape}\")\n",
    "\n",
    "    # Reduce the dataset size by taking a subset (e.g., first 10,000 rows)\n",
    "    final_features = final_features.head(5000)\n",
    "    log(f\"Reduced final features dataset shape: {final_features.shape}\")\n",
    "    log(\"First 15 rows of the reduced final features dataset:\")\n",
    "    print(final_features.head(15))\n",
    "except FileNotFoundError:\n",
    "    log(f\"File not found: {final_features_path}\")\n",
    "    raise\n",
    "\n",
    "# Load the final script features dataset\n",
    "log(\"Loading final script features dataset...\")\n",
    "try:\n",
    "    final_script_features = pd.read_csv(final_script_features_path)\n",
    "    log(f\"Final script features dataset loaded with shape: {final_script_features.shape}\")\n",
    "    log(\"First 15 rows of the final script features dataset:\")\n",
    "    print(final_script_features.head(15))\n",
    "except FileNotFoundError:\n",
    "    log(f\"File not found: {final_script_features_path}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee48fdc3-7a6c-492b-a591-1b1008e3f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:17:07,635] [INFO]: Splitting data into training and testing sets for IMDb and TMDb features...\n",
      "[2024-08-02 18:17:07,640] [INFO]: Training set shape: X_train: (3000, 49), y_train: (3000,)\n",
      "[2024-08-02 18:17:07,641] [INFO]: Testing set shape: X_test: (2000, 49), y_test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data for IMDb and TMDb features (Random Forest Model)\n",
    "log(\"Splitting data into training and testing sets for IMDb and TMDb features...\")\n",
    "X = final_features.drop(columns=['averageRating'])\n",
    "y = final_features['averageRating']  # Assuming this is the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "log(f\"Training set shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "log(f\"Testing set shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d49c2e-b244-4cd5-99b2-beb0497859a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:17:07,649] [INFO]: Starting hyperparameter tuning with GridSearchCV...\n",
      "[2024-08-02 18:17:07,649] [INFO]: Fitting GridSearchCV model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:19:15,105] [INFO]: Best parameters found: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "[2024-08-02 18:19:15,106] [INFO]: Building the Random Forest Regressor with best parameters...\n",
      "[2024-08-02 18:19:16,011] [INFO]: Random Forest model training completed.\n",
      "[2024-08-02 18:19:16,012] [INFO]: Evaluating the Random Forest model...\n",
      "[2024-08-02 18:19:16,041] [INFO]: Random Forest Model RMSE: 0.9451587976541747\n",
      "[2024-08-02 18:19:16,042] [INFO]: R^2 Score: 0.20121054837798746\n",
      "[2024-08-02 18:19:16,042] [INFO]: Calculating feature importances...\n",
      "[2024-08-02 18:19:16,052] [INFO]: Top 15 features by importance:\n",
      "[2024-08-02 18:19:16,053] [INFO]: \n",
      "                  feature  importance\n",
      "                 numVotes    0.366392\n",
      "                startYear    0.322989\n",
      "runtimeMinutes_normalized    0.158757\n",
      "                adventure    0.042325\n",
      "                   comedy    0.021774\n",
      "                    drama    0.018012\n",
      "                  western    0.016994\n",
      "                   action    0.007674\n",
      "                     news    0.005968\n",
      "                  romance    0.005717\n",
      "                    short    0.005581\n",
      "                  fantasy    0.004648\n",
      "                animation    0.003605\n",
      "              documentary    0.003372\n",
      "                    crime    0.003239\n",
      "[2024-08-02 18:19:16,054] [INFO]: Saving the Random Forest model to D:\\manav\\Documents\\Engineering\\Masters\\EE8206\\Project\\MovieSuccessPredictor\\models\\randomforest\\random_forest_model.pkl...\n",
      "[2024-08-02 18:19:16,060] [INFO]: Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ... (previous code for loading data and defining models)\n",
    "\n",
    "# Hyperparameter Tuning using GridSearchCV\n",
    "log(\"Starting hyperparameter tuning with GridSearchCV...\")\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Initializing GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fitting the GridSearchCV model\n",
    "log(\"Fitting GridSearchCV model...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extracting the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "log(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Building the Random Forest Model with best parameters\n",
    "log(\"Building the Random Forest Regressor with best parameters...\")\n",
    "rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "log(\"Random Forest model training completed.\")\n",
    "\n",
    "# Evaluate the Random Forest Model\n",
    "log(\"Evaluating the Random Forest model...\")\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Log evaluation metrics\n",
    "log(f\"Random Forest Model RMSE: {rmse_rf}\")\n",
    "log(f\"R^2 Score: {r2_rf}\")\n",
    "\n",
    "# Feature Importance\n",
    "log(\"Calculating feature importances...\")\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances})\n",
    "feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "\n",
    "log(\"Top 15 features by importance:\")\n",
    "log(\"\\n\" + feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Ensure the model directory exists\n",
    "model_dir = os.path.join(base_dir, 'models', 'randomforest')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model for future use\n",
    "model_save_path = os.path.join(model_dir, 'random_forest_model.pkl')\n",
    "log(f\"Saving the Random Forest model to {model_save_path}...\")\n",
    "with open(model_save_path, 'wb') as file:\n",
    "    pickle.dump(rf_model, file)\n",
    "log(\"Model saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8ef54b1-5443-4ead-b7b4-f6701a108b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:19:16,065] [INFO]: Preparing data for the Neural Network (Script Features)...\n",
      "[2024-08-02 18:19:16,151] [INFO]: Combining tokenized scripts with additional features...\n",
      "[2024-08-02 18:19:16,153] [INFO]: Combined script feature shape: (930, 514)\n"
     ]
    }
   ],
   "source": [
    "# Function to correctly parse the tokenized script padded data\n",
    "def parse_tokenized_script_padded(token_str):\n",
    "    try:\n",
    "        # Remove any characters that might cause issues and convert to a list\n",
    "        token_str = token_str.replace('\\n', '').replace('[', ' ').replace(']', ' ').strip()\n",
    "        token_list = list(map(int, token_str.split()))\n",
    "        return token_list\n",
    "    except Exception as e:\n",
    "        log(f\"Error parsing tokenized_script_padded entry: {e}\")\n",
    "        return None  # Return None for problematic entries\n",
    "\n",
    "# Preparing data for the Neural Network\n",
    "log(\"Preparing data for the Neural Network (Script Features)...\")\n",
    "try:\n",
    "    X_script = np.array(final_script_features['tokenized_script_padded'].apply(parse_tokenized_script_padded).tolist())\n",
    "    # Removing any None entries that could have occurred during parsing\n",
    "    X_script = np.array([x for x in X_script if x is not None])\n",
    "except Exception as e:\n",
    "    log(f\"Exception occurred during conversion: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check for any issues with the conversion and handle accordingly\n",
    "if X_script.ndim == 1 or X_script.shape[1] != 512:  # Check for correct dimensions\n",
    "    log(\"Error in parsing tokenized_script_padded data. Ensure data is correctly formatted as lists of length 512.\")\n",
    "    raise ValueError(\"Incorrect data format for tokenized_script_padded.\")\n",
    "\n",
    "# Combining tokenized scripts with additional features\n",
    "log(\"Combining tokenized scripts with additional features...\")\n",
    "additional_features = final_script_features[['sentiment', 'readabilityScore']].values\n",
    "X_script_combined = np.concatenate([X_script, additional_features], axis=1)\n",
    "\n",
    "log(f\"Combined script feature shape: {X_script_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e270a9c-43c1-4ff7-a95e-f5835488f82d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:19:16,159] [INFO]: Loading BERT tokenizer and model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:19:16,518] [WARNING]: From C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "[2024-08-02 18:19:17,608] [INFO]: Converting tokenized script data to BERT-compatible input...\n",
      "[2024-08-02 18:19:17,666] [INFO]: Extracting BERT embeddings for tokenized scripts in batches...\n",
      "[2024-08-02 18:19:17,667] [INFO]: Processing batch 1/117...\n",
      "[2024-08-02 18:19:20,019] [INFO]: Processing batch 2/117...\n",
      "[2024-08-02 18:19:22,381] [INFO]: Processing batch 3/117...\n",
      "[2024-08-02 18:19:24,749] [INFO]: Processing batch 4/117...\n",
      "[2024-08-02 18:19:27,128] [INFO]: Processing batch 5/117...\n",
      "[2024-08-02 18:19:29,482] [INFO]: Processing batch 6/117...\n",
      "[2024-08-02 18:19:31,841] [INFO]: Processing batch 7/117...\n",
      "[2024-08-02 18:19:34,206] [INFO]: Processing batch 8/117...\n",
      "[2024-08-02 18:19:36,560] [INFO]: Processing batch 9/117...\n",
      "[2024-08-02 18:19:38,939] [INFO]: Processing batch 10/117...\n",
      "[2024-08-02 18:19:41,297] [INFO]: Processing batch 11/117...\n",
      "[2024-08-02 18:19:43,663] [INFO]: Processing batch 12/117...\n",
      "[2024-08-02 18:19:46,029] [INFO]: Processing batch 13/117...\n",
      "[2024-08-02 18:19:48,423] [INFO]: Processing batch 14/117...\n",
      "[2024-08-02 18:19:50,874] [INFO]: Processing batch 15/117...\n",
      "[2024-08-02 18:19:53,236] [INFO]: Processing batch 16/117...\n",
      "[2024-08-02 18:19:55,590] [INFO]: Processing batch 17/117...\n",
      "[2024-08-02 18:19:57,948] [INFO]: Processing batch 18/117...\n",
      "[2024-08-02 18:20:00,323] [INFO]: Processing batch 19/117...\n",
      "[2024-08-02 18:20:02,676] [INFO]: Processing batch 20/117...\n",
      "[2024-08-02 18:20:05,049] [INFO]: Processing batch 21/117...\n",
      "[2024-08-02 18:20:07,406] [INFO]: Processing batch 22/117...\n",
      "[2024-08-02 18:20:09,784] [INFO]: Processing batch 23/117...\n",
      "[2024-08-02 18:20:12,136] [INFO]: Processing batch 24/117...\n",
      "[2024-08-02 18:20:14,557] [INFO]: Processing batch 25/117...\n",
      "[2024-08-02 18:20:16,914] [INFO]: Processing batch 26/117...\n",
      "[2024-08-02 18:20:19,274] [INFO]: Processing batch 27/117...\n",
      "[2024-08-02 18:20:21,621] [INFO]: Processing batch 28/117...\n",
      "[2024-08-02 18:20:23,988] [INFO]: Processing batch 29/117...\n",
      "[2024-08-02 18:20:26,336] [INFO]: Processing batch 30/117...\n",
      "[2024-08-02 18:20:28,695] [INFO]: Processing batch 31/117...\n",
      "[2024-08-02 18:20:31,047] [INFO]: Processing batch 32/117...\n",
      "[2024-08-02 18:20:33,408] [INFO]: Processing batch 33/117...\n",
      "[2024-08-02 18:20:35,821] [INFO]: Processing batch 34/117...\n",
      "[2024-08-02 18:20:38,248] [INFO]: Processing batch 35/117...\n",
      "[2024-08-02 18:20:40,653] [INFO]: Processing batch 36/117...\n",
      "[2024-08-02 18:20:43,098] [INFO]: Processing batch 37/117...\n",
      "[2024-08-02 18:20:45,557] [INFO]: Processing batch 38/117...\n",
      "[2024-08-02 18:20:47,974] [INFO]: Processing batch 39/117...\n",
      "[2024-08-02 18:20:50,382] [INFO]: Processing batch 40/117...\n",
      "[2024-08-02 18:20:52,829] [INFO]: Processing batch 41/117...\n",
      "[2024-08-02 18:20:55,186] [INFO]: Processing batch 42/117...\n",
      "[2024-08-02 18:20:57,536] [INFO]: Processing batch 43/117...\n",
      "[2024-08-02 18:20:59,894] [INFO]: Processing batch 44/117...\n",
      "[2024-08-02 18:21:02,250] [INFO]: Processing batch 45/117...\n",
      "[2024-08-02 18:21:04,613] [INFO]: Processing batch 46/117...\n",
      "[2024-08-02 18:21:06,970] [INFO]: Processing batch 47/117...\n",
      "[2024-08-02 18:21:09,324] [INFO]: Processing batch 48/117...\n",
      "[2024-08-02 18:21:11,680] [INFO]: Processing batch 49/117...\n",
      "[2024-08-02 18:21:14,041] [INFO]: Processing batch 50/117...\n",
      "[2024-08-02 18:21:16,431] [INFO]: Processing batch 51/117...\n",
      "[2024-08-02 18:21:18,794] [INFO]: Processing batch 52/117...\n",
      "[2024-08-02 18:21:21,145] [INFO]: Processing batch 53/117...\n",
      "[2024-08-02 18:21:23,500] [INFO]: Processing batch 54/117...\n",
      "[2024-08-02 18:21:25,862] [INFO]: Processing batch 55/117...\n",
      "[2024-08-02 18:21:28,233] [INFO]: Processing batch 56/117...\n",
      "[2024-08-02 18:21:30,591] [INFO]: Processing batch 57/117...\n",
      "[2024-08-02 18:21:32,958] [INFO]: Processing batch 58/117...\n",
      "[2024-08-02 18:21:35,323] [INFO]: Processing batch 59/117...\n",
      "[2024-08-02 18:21:37,706] [INFO]: Processing batch 60/117...\n",
      "[2024-08-02 18:21:40,065] [INFO]: Processing batch 61/117...\n",
      "[2024-08-02 18:21:42,425] [INFO]: Processing batch 62/117...\n",
      "[2024-08-02 18:21:44,783] [INFO]: Processing batch 63/117...\n",
      "[2024-08-02 18:21:47,136] [INFO]: Processing batch 64/117...\n",
      "[2024-08-02 18:21:49,506] [INFO]: Processing batch 65/117...\n",
      "[2024-08-02 18:21:51,890] [INFO]: Processing batch 66/117...\n",
      "[2024-08-02 18:21:54,294] [INFO]: Processing batch 67/117...\n",
      "[2024-08-02 18:21:56,641] [INFO]: Processing batch 68/117...\n",
      "[2024-08-02 18:21:59,008] [INFO]: Processing batch 69/117...\n",
      "[2024-08-02 18:22:01,373] [INFO]: Processing batch 70/117...\n",
      "[2024-08-02 18:22:03,735] [INFO]: Processing batch 71/117...\n",
      "[2024-08-02 18:22:06,090] [INFO]: Processing batch 72/117...\n",
      "[2024-08-02 18:22:08,450] [INFO]: Processing batch 73/117...\n",
      "[2024-08-02 18:22:10,813] [INFO]: Processing batch 74/117...\n",
      "[2024-08-02 18:22:13,168] [INFO]: Processing batch 75/117...\n",
      "[2024-08-02 18:22:15,523] [INFO]: Processing batch 76/117...\n",
      "[2024-08-02 18:22:17,876] [INFO]: Processing batch 77/117...\n",
      "[2024-08-02 18:22:20,233] [INFO]: Processing batch 78/117...\n",
      "[2024-08-02 18:22:22,579] [INFO]: Processing batch 79/117...\n",
      "[2024-08-02 18:22:24,945] [INFO]: Processing batch 80/117...\n",
      "[2024-08-02 18:22:27,292] [INFO]: Processing batch 81/117...\n",
      "[2024-08-02 18:22:29,652] [INFO]: Processing batch 82/117...\n",
      "[2024-08-02 18:22:32,006] [INFO]: Processing batch 83/117...\n",
      "[2024-08-02 18:22:34,375] [INFO]: Processing batch 84/117...\n",
      "[2024-08-02 18:22:36,730] [INFO]: Processing batch 85/117...\n",
      "[2024-08-02 18:22:39,107] [INFO]: Processing batch 86/117...\n",
      "[2024-08-02 18:22:41,459] [INFO]: Processing batch 87/117...\n",
      "[2024-08-02 18:22:43,822] [INFO]: Processing batch 88/117...\n",
      "[2024-08-02 18:22:46,177] [INFO]: Processing batch 89/117...\n",
      "[2024-08-02 18:22:48,532] [INFO]: Processing batch 90/117...\n",
      "[2024-08-02 18:22:50,890] [INFO]: Processing batch 91/117...\n",
      "[2024-08-02 18:22:53,445] [INFO]: Processing batch 92/117...\n",
      "[2024-08-02 18:22:55,861] [INFO]: Processing batch 93/117...\n",
      "[2024-08-02 18:22:58,216] [INFO]: Processing batch 94/117...\n",
      "[2024-08-02 18:23:00,703] [INFO]: Processing batch 95/117...\n",
      "[2024-08-02 18:23:03,086] [INFO]: Processing batch 96/117...\n",
      "[2024-08-02 18:23:05,454] [INFO]: Processing batch 97/117...\n",
      "[2024-08-02 18:23:07,819] [INFO]: Processing batch 98/117...\n",
      "[2024-08-02 18:23:10,171] [INFO]: Processing batch 99/117...\n",
      "[2024-08-02 18:23:12,597] [INFO]: Processing batch 100/117...\n",
      "[2024-08-02 18:23:15,022] [INFO]: Processing batch 101/117...\n",
      "[2024-08-02 18:23:17,409] [INFO]: Processing batch 102/117...\n",
      "[2024-08-02 18:23:19,774] [INFO]: Processing batch 103/117...\n",
      "[2024-08-02 18:23:22,128] [INFO]: Processing batch 104/117...\n",
      "[2024-08-02 18:23:24,632] [INFO]: Processing batch 105/117...\n",
      "[2024-08-02 18:23:27,242] [INFO]: Processing batch 106/117...\n",
      "[2024-08-02 18:23:29,809] [INFO]: Processing batch 107/117...\n",
      "[2024-08-02 18:23:32,173] [INFO]: Processing batch 108/117...\n",
      "[2024-08-02 18:23:34,535] [INFO]: Processing batch 109/117...\n",
      "[2024-08-02 18:23:36,904] [INFO]: Processing batch 110/117...\n",
      "[2024-08-02 18:23:39,311] [INFO]: Processing batch 111/117...\n",
      "[2024-08-02 18:23:42,014] [INFO]: Processing batch 112/117...\n",
      "[2024-08-02 18:23:44,892] [INFO]: Processing batch 113/117...\n",
      "[2024-08-02 18:23:47,696] [INFO]: Processing batch 114/117...\n",
      "[2024-08-02 18:23:50,409] [INFO]: Processing batch 115/117...\n",
      "[2024-08-02 18:23:52,932] [INFO]: Processing batch 116/117...\n",
      "[2024-08-02 18:23:55,476] [INFO]: Processing batch 117/117...\n",
      "[2024-08-02 18:23:56,301] [INFO]: BERT embeddings shape: (930, 512, 768)\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "log(\"Loading BERT tokenizer and model...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define a function to safely convert the tokenized script data into input suitable for BERT\n",
    "def prepare_bert_input(tokenized_data):\n",
    "    try:\n",
    "        # Ensure the list is of integers and truncate/pad to the maximum BERT input length of 512\n",
    "        return tokenized_data[:512] + [0] * (512 - len(tokenized_data)) if len(tokenized_data) < 512 else tokenized_data[:512]\n",
    "    except Exception as e:\n",
    "        log(f\"Error in preparing BERT input: {e}\")\n",
    "        return [0] * 512  # Return a zero-padded list on error\n",
    "\n",
    "# Convert the 'tokenized_script_padded' column to a list of lists suitable for BERT\n",
    "log(\"Converting tokenized script data to BERT-compatible input...\")\n",
    "X_script_bert_input = final_script_features['tokenized_script_padded'].apply(parse_tokenized_script_padded).apply(prepare_bert_input).tolist()\n",
    "\n",
    "# Batch processing for extracting BERT embeddings\n",
    "def batch_process_bert_embeddings(tokenized_scripts, batch_size=8):\n",
    "    all_embeddings = []\n",
    "    num_batches = len(tokenized_scripts) // batch_size + (1 if len(tokenized_scripts) % batch_size != 0 else 0)\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        log(f\"Processing batch {i + 1}/{num_batches}...\")\n",
    "        batch = tokenized_scripts[i * batch_size:(i + 1) * batch_size]\n",
    "        input_ids_batch = tf.convert_to_tensor(batch, dtype=tf.int32)\n",
    "        batch_embeddings = get_bert_embeddings(input_ids_batch)\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "    \n",
    "    # Concatenate all batches into a single tensor\n",
    "    return tf.concat(all_embeddings, axis=0)\n",
    "\n",
    "# Function to extract BERT embeddings for a batch of tokenized scripts\n",
    "def get_bert_embeddings(input_ids):\n",
    "    # Pass through BERT model and get the last hidden state\n",
    "    outputs = bert_model(input_ids)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "# Extracting BERT embeddings for the tokenized scripts in batches\n",
    "log(\"Extracting BERT embeddings for tokenized scripts in batches...\")\n",
    "bert_embeddings = batch_process_bert_embeddings(X_script_bert_input, batch_size=8)\n",
    "log(f\"BERT embeddings shape: {bert_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6abe39-cb27-4762-ae7d-cbe4fc483d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:37:44,394] [INFO]: Converting BERT embeddings to numpy array and ensuring correct shape...\n",
      "[2024-08-02 18:37:44,741] [INFO]: Reducing the dimensionality of BERT embeddings using mean pooling...\n",
      "[2024-08-02 18:37:44,841] [INFO]: Concatenating BERT embeddings with additional features...\n",
      "[2024-08-02 18:37:44,844] [INFO]: Combined feature shape: (930, 769)\n",
      "[2024-08-02 18:37:44,844] [INFO]: Defining target variable...\n",
      "[2024-08-02 18:37:44,844] [INFO]: Splitting the data into training and testing sets...\n",
      "[2024-08-02 18:37:44,847] [INFO]: Training set shape: X_train: (744, 769), y_train: (744,)\n",
      "[2024-08-02 18:37:44,848] [INFO]: Testing set shape: X_test: (186, 769), y_test: (186,)\n",
      "[2024-08-02 18:37:44,848] [INFO]: Building the Neural Network model with a custom structure...\n",
      "[2024-08-02 18:37:44,873] [INFO]: Compiling the model...\n",
      "[2024-08-02 18:37:44,880] [INFO]: Model summary:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 128)               98560     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 106881 (417.50 KB)\n",
      "Trainable params: 106881 (417.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:37:44,886] [INFO]: Training the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "24/24 [==============================] - 1s 6ms/step - loss: 3.1418 - mean_squared_error: 3.1418 - val_loss: 0.1455 - val_mean_squared_error: 0.1455\n",
      "Epoch 2/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.1361 - mean_squared_error: 0.1361 - val_loss: 0.0625 - val_mean_squared_error: 0.0625\n",
      "Epoch 3/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 4/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 5/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 6/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0057 - val_mean_squared_error: 0.0057\n",
      "Epoch 7/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0053 - val_mean_squared_error: 0.0053\n",
      "Epoch 8/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0049 - val_mean_squared_error: 0.0049\n",
      "Epoch 9/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0046 - val_mean_squared_error: 0.0046\n",
      "Epoch 10/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0044 - val_mean_squared_error: 0.0044\n",
      "Epoch 11/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0040 - mean_squared_error: 0.0040 - val_loss: 0.0042 - val_mean_squared_error: 0.0042\n",
      "Epoch 12/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0041 - mean_squared_error: 0.0041 - val_loss: 0.0041 - val_mean_squared_error: 0.0041\n",
      "Epoch 13/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0035 - mean_squared_error: 0.0035 - val_loss: 0.0037 - val_mean_squared_error: 0.0037\n",
      "Epoch 14/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0032 - mean_squared_error: 0.0032 - val_loss: 0.0035 - val_mean_squared_error: 0.0035\n",
      "Epoch 15/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0030 - mean_squared_error: 0.0030 - val_loss: 0.0034 - val_mean_squared_error: 0.0034\n",
      "Epoch 16/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0033 - val_mean_squared_error: 0.0033\n",
      "Epoch 17/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0028 - mean_squared_error: 0.0028 - val_loss: 0.0031 - val_mean_squared_error: 0.0031\n",
      "Epoch 18/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0025 - mean_squared_error: 0.0025 - val_loss: 0.0032 - val_mean_squared_error: 0.0032\n",
      "Epoch 19/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0026 - mean_squared_error: 0.0026 - val_loss: 0.0029 - val_mean_squared_error: 0.0029\n",
      "Epoch 20/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0024 - mean_squared_error: 0.0024 - val_loss: 0.0028 - val_mean_squared_error: 0.0028\n",
      "Epoch 21/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 22/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0021 - mean_squared_error: 0.0021 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 23/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0023 - mean_squared_error: 0.0023 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 24/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 25/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0022 - mean_squared_error: 0.0022 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 26/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 27/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0020 - mean_squared_error: 0.0020 - val_loss: 0.0025 - val_mean_squared_error: 0.0025\n",
      "Epoch 28/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 29/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0026 - val_mean_squared_error: 0.0026\n",
      "Epoch 30/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 31/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 32/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 33/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 34/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0016 - mean_squared_error: 0.0016 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 35/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0027 - val_mean_squared_error: 0.0027\n",
      "Epoch 36/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n",
      "Epoch 37/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0021 - val_mean_squared_error: 0.0021\n",
      "Epoch 38/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0018 - mean_squared_error: 0.0018 - val_loss: 0.0020 - val_mean_squared_error: 0.0020\n",
      "Epoch 39/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0015 - mean_squared_error: 0.0015 - val_loss: 0.0023 - val_mean_squared_error: 0.0023\n",
      "Epoch 40/40\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0017 - mean_squared_error: 0.0017 - val_loss: 0.0022 - val_mean_squared_error: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:37:47,374] [INFO]: Evaluating the model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 997us/step - loss: 0.0022 - mean_squared_error: 0.0022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 18:37:47,424] [INFO]: Test MSE: 0.0021939983125776052\n",
      "[2024-08-02 18:37:47,425] [INFO]: Saving the trained sentiment model to sentiment_model_v2.h5...\n",
      "C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "[2024-08-02 18:37:47,445] [INFO]: Sentiment model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Convert BERT embeddings to numpy array and ensure correct shape\n",
    "log(\"Converting BERT embeddings to numpy array and ensuring correct shape...\")\n",
    "bert_embeddings_np = bert_embeddings.numpy()\n",
    "\n",
    "# Reducing the dimensionality of BERT embeddings using mean pooling\n",
    "log(\"Reducing the dimensionality of BERT embeddings using mean pooling...\")\n",
    "bert_embeddings_mean = np.mean(bert_embeddings_np, axis=1)  # Shape: (batch_size, embedding_size)\n",
    "\n",
    "# Concatenate BERT embeddings with additional features\n",
    "log(\"Concatenating BERT embeddings with additional features...\")\n",
    "additional_features = final_script_features[['readabilityScore']].values  # Exclude 'sentiment' from features\n",
    "X_combined = np.concatenate([bert_embeddings_mean, additional_features], axis=1)\n",
    "log(f\"Combined feature shape: {X_combined.shape}\")\n",
    "\n",
    "# Define target variable (sentiment score)\n",
    "log(\"Defining target variable...\")\n",
    "y = final_script_features['sentiment'].values\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "log(\"Splitting the data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "log(f\"Training set shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "log(f\"Testing set shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Building the Neural Network model\n",
    "firstLayerNeurons = 128\n",
    "secondLayerNeurons = 64\n",
    "\n",
    "log(\"Building the Neural Network model with a custom structure...\")\n",
    "model = keras.Sequential([\n",
    "    # First dense layer with BERT embeddings and additional features as input\n",
    "    keras.layers.Dense(firstLayerNeurons, activation=tf.nn.relu, input_shape=(X_combined.shape[1],)),\n",
    "    \n",
    "    # Second dense layer\n",
    "    keras.layers.Dense(secondLayerNeurons, activation=tf.nn.relu),\n",
    "    \n",
    "    # Output layer for regression (predicting sentiment score)\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "log(\"Compiling the model...\")\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "# Model summary\n",
    "log(\"Model summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "log(\"Training the model...\")\n",
    "model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "log(\"Evaluating the model...\")\n",
    "test_loss, test_mse = model.evaluate(X_test, y_test)\n",
    "log(f\"Test MSE: {test_mse}\")\n",
    "\n",
    "# Save the trained sentiment model\n",
    "sentiment_model_save_path = 'sentiment_model_v2.h5'\n",
    "log(f\"Saving the trained sentiment model to {sentiment_model_save_path}...\")\n",
    "model.save(sentiment_model_save_path)\n",
    "log(\"Sentiment model saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f419f76-8f00-4ab4-aabf-3de2f320ef71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 19:13:01,954] [INFO]: Loading sentiment analysis model...\n",
      "[2024-08-02 19:13:01,993] [INFO]: Loading BERT tokenizer...\n",
      "[2024-08-02 19:13:02,153] [INFO]: Fetching script from URL...\n",
      "[2024-08-02 19:13:02,294] [INFO]: Script fetched successfully.\n",
      "[2024-08-02 19:13:02,295] [INFO]: Tokenizing the script for BERT...\n",
      "[2024-08-02 19:13:02,708] [INFO]: Extracting BERT embeddings...\n",
      "[2024-08-02 19:13:03,133] [INFO]: Predicting sentiment score...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 19:13:03,204] [INFO]: Predicted Sentiment Score for the script: [[0.10024708]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow import keras\n",
    "\n",
    "# Ensure the sentiment model is loaded\n",
    "log(\"Loading sentiment analysis model...\")\n",
    "sentiment_model = keras.models.load_model('sentiment_model_v2.h5')\n",
    "\n",
    "# Load BERT tokenizer (assuming the model is already loaded as 'bert_model')\n",
    "log(\"Loading BERT tokenizer...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def prepare_script_for_bert(script):\n",
    "    \"\"\"Tokenizes and prepares the script for BERT input.\"\"\"\n",
    "    log(\"Tokenizing the script for BERT...\")\n",
    "    tokenized = bert_tokenizer.encode_plus(\n",
    "        script,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "    return tokenized['input_ids'][0].numpy()\n",
    "\n",
    "def extract_bert_embeddings(input_ids):\n",
    "    \"\"\"Extracts BERT embeddings for the input tokens.\"\"\"\n",
    "    log(\"Extracting BERT embeddings...\")\n",
    "    outputs = bert_model(input_ids)\n",
    "    return outputs.last_hidden_state\n",
    "\n",
    "def compute_readability(script):\n",
    "    \"\"\"Computes readability score for the given script.\"\"\"\n",
    "    # Placeholder function: Replace with your actual readability score calculation\n",
    "    return 5.0  # Example score\n",
    "\n",
    "def get_script_from_url(url):\n",
    "    \"\"\"Fetches script from the given URL and returns the text.\"\"\"\n",
    "    log(\"Fetching script from URL...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        script_text = soup.get_text()\n",
    "        log(\"Script fetched successfully.\")\n",
    "        return script_text\n",
    "    else:\n",
    "        log(f\"Failed to fetch the script. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def predict_script_sentiment(script):\n",
    "    \"\"\"Predicts the sentiment score of a movie script.\"\"\"\n",
    "    # Preprocess the script for BERT input\n",
    "    tokenized_script = prepare_script_for_bert(script)\n",
    "    input_ids = tf.convert_to_tensor([tokenized_script], dtype=tf.int32)\n",
    "\n",
    "    # Extract BERT embeddings\n",
    "    bert_embeddings = extract_bert_embeddings(input_ids)\n",
    "    bert_embeddings_mean = np.mean(bert_embeddings.numpy(), axis=1)\n",
    "\n",
    "    # Prepare additional features\n",
    "    readability_score = compute_readability(script)\n",
    "    additional_features = np.array([[readability_score]])\n",
    "\n",
    "    # Combine BERT embeddings and additional features\n",
    "    X_input = np.concatenate([bert_embeddings_mean, additional_features], axis=1)\n",
    "\n",
    "    # Predict sentiment\n",
    "    log(\"Predicting sentiment score...\")\n",
    "    predicted_sentiment = sentiment_model.predict(X_input)\n",
    "    return predicted_sentiment\n",
    "\n",
    "# Fetch the script from the given URL and predict its sentiment\n",
    "script_url = \"https://imsdb.com/scripts/Tenet.html\"\n",
    "script_text = get_script_from_url(script_url)\n",
    "if script_text:\n",
    "    predicted_sentiment = predict_script_sentiment(script_text)\n",
    "    log(f\"Predicted Sentiment Score for the script: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6cfd0c1-09ce-456a-bad8-4bc2a74333a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 19:02:03,135] [INFO]: Loading the Random Forest model...\n",
      "[2024-08-02 19:02:03,146] [INFO]: Predicting movie success rating...\n",
      "C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[2024-08-02 19:02:03,154] [INFO]: Predicted Movie Rating: [6.11832094]\n",
      "[2024-08-02 19:02:03,155] [INFO]: Fetching script from URL...\n",
      "[2024-08-02 19:02:03,429] [INFO]: Script fetched successfully.\n"
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the Random Forest model\n",
    "log(\"Loading the Random Forest model...\")\n",
    "model_path = 'D:/manav/Documents/Engineering/Masters/EE8206/Project/MovieSuccessPredictor/models/randomforest/random_forest_model.pkl'\n",
    "try:\n",
    "    with open(model_path, 'rb') as file:\n",
    "        random_forest_model = pickle.load(file)\n",
    "except PermissionError:\n",
    "    log(\"Permission denied: unable to access the model file.\")\n",
    "except FileNotFoundError:\n",
    "    log(f\"File not found: {model_path}\")\n",
    "except Exception as e:\n",
    "    log(f\"An error occurred while loading the model: {e}\")\n",
    "\n",
    "def prepare_features_for_rf(features):\n",
    "    \"\"\"Prepares features for the Random Forest model.\"\"\"\n",
    "    return np.array(features)\n",
    "\n",
    "def predict_movie_rating(features):\n",
    "    \"\"\"Predicts the movie success rating using the Random Forest model.\"\"\"\n",
    "    prepared_features = prepare_features_for_rf(features)\n",
    "    log(\"Predicting movie success rating...\")\n",
    "    predicted_rating = random_forest_model.predict([prepared_features])\n",
    "    return predicted_rating\n",
    "\n",
    "def get_script_from_url(url):\n",
    "    \"\"\"Fetches script from the given URL and returns the text.\"\"\"\n",
    "    log(\"Fetching script from URL...\")\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        script_text = soup.get_text()\n",
    "        log(\"Script fetched successfully.\")\n",
    "        return script_text\n",
    "    else:\n",
    "        log(f\"Failed to fetch the script. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Hardcoded sample features from final_features.csv (excluding the target 'averageRating')\n",
    "sample_features = [\n",
    "    2063,  # numVotes\n",
    "    1.6818028927009755e-05,  # runtimeMinutes_normalized\n",
    "    1894.0,  # startYear\n",
    "    False,  # titleType_movie\n",
    "    True,  # titleType_short\n",
    "    False,  # titleType_tvEpisode\n",
    "    False,  # titleType_tvMiniSeries\n",
    "    False,  # titleType_tvMovie\n",
    "    False,  # titleType_tvSeries\n",
    "    False,  # titleType_tvShort\n",
    "    False,  # titleType_tvSpecial\n",
    "    False,  # titleType_video\n",
    "    False,  # titleType_videoGame\n",
    "    0,  # action\n",
    "    0,  # adult\n",
    "    0,  # adventure\n",
    "    0,  # animation\n",
    "    0,  # biography\n",
    "    0,  # comedy\n",
    "    0,  # crime\n",
    "    1,  # documentary\n",
    "    0,  # drama\n",
    "    0,  # family\n",
    "    0,  # fantasy\n",
    "    0,  # film-noir\n",
    "    0,  # game-show\n",
    "    0,  # history\n",
    "    0,  # horror\n",
    "    0,  # music\n",
    "    0,  # musical\n",
    "    0,  # mystery\n",
    "    0,  # news\n",
    "    0,  # reality-tv\n",
    "    0,  # romance\n",
    "    0,  # sci-fi\n",
    "    0,  # short\n",
    "    0,  # sport\n",
    "    0,  # talk-show\n",
    "    0,  # thriller\n",
    "    0,  # war\n",
    "    0,  # western\n",
    "    True,  # season_Winter\n",
    "    0.0,  # directorPopularity\n",
    "    0.0,  # wordCount\n",
    "    0.0,  # sentiment\n",
    "    0.0,  # readabilityScore\n",
    "    False,  # comedy_indicator\n",
    "    False,  # horror_indicator\n",
    "    False  # action_indicator\n",
    "]\n",
    "\n",
    "# Ensure the feature count matches the model's expectation\n",
    "expected_feature_count = 49  # Change this to match your model's expected input\n",
    "if len(sample_features) != expected_feature_count:\n",
    "    log(f\"Feature count mismatch. Expected {expected_feature_count}, got {len(sample_features)}.\")\n",
    "    raise ValueError(\"Feature count mismatch\")\n",
    "\n",
    "# Predict the rating using the Random Forest model\n",
    "predicted_rating = predict_movie_rating(sample_features)\n",
    "log(f\"Predicted Movie Rating: {predicted_rating}\")\n",
    "\n",
    "# Fetch the script from the given URL\n",
    "script_url = \"https://imsdb.com/scripts/Tenet.html\"\n",
    "script_text = get_script_from_url(script_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b99756b-228c-40ff-b710-353b5fbce788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-08-02 19:13:10,545] [INFO]: Saving the models...\n",
      "C:\\Users\\manav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "[2024-08-02 19:13:10,577] [INFO]: Models saved successfully.\n",
      "[2024-08-02 19:13:10,578] [INFO]: Loading the models...\n",
      "[2024-08-02 19:13:10,637] [INFO]: Models loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save the sentiment model and Random Forest model after training\n",
    "log(\"Saving the models...\")\n",
    "sentiment_model.save('sentiment_model.h5')\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(random_forest_model, f)\n",
    "log(\"Models saved successfully.\")\n",
    "\n",
    "# Load the models for prediction\n",
    "log(\"Loading the models...\")\n",
    "sentiment_model = keras.models.load_model('sentiment_model.h5')\n",
    "with open('random_forest_model.pkl', 'rb') as f:\n",
    "    random_forest_model = pickle.load(f)\n",
    "log(\"Models loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12f5d0b-02ff-4374-adfe-e47b6864ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
